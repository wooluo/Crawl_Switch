from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import json
from datetime import datetime
import time
import random
import pytz
import os

# é…ç½®å‚æ•°
CONFIG = {
    "base_url": "https://www.gamer520.com/gameswitch",
    "pages_to_crawl": 5,
    "timeout": 45000,  # 45ç§’è¶…æ—¶
    "max_retries": 3,  # æœ€å¤§é‡è¯•æ¬¡æ•°
    "retry_delay": 5,  # é‡è¯•å»¶è¿Ÿ(ç§’)
    "min_delay": 3,    # æœ€å°è¯·æ±‚é—´éš”
    "max_delay": 8     # æœ€å¤§è¯·æ±‚é—´éš”
}

# ç”¨æˆ·ä»£ç†åˆ—è¡¨
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0"
]

def get_timestamp():
    """è·å–å½“å‰æ—¶é—´æˆ³"""
    return datetime.now(pytz.timezone("Asia/Shanghai")).strftime('%Y%m%d_%H%M%S')

def save_results(data):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    timestamp = get_timestamp()
    json_filename = f"results_{timestamp}.json"
    md_filename = f"switch_news_{timestamp}.md"
    current_time = datetime.now(pytz.timezone("Asia/Shanghai")).strftime('%Y-%m-%d %H:%M')

    # ä¿å­˜JSONæ–‡ä»¶
    with open(json_filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    # ç”ŸæˆMarkdownæ–‡ä»¶
    with open(md_filename, "w", encoding="utf-8") as f:
        f.write(f"# Nintendo Switch æ¸¸æˆä¿¡æ¯\næ›´æ–°æ—¶é—´ï¼š{current_time} (UTC+8)\n\n")
        if data:
            f.write(f"âœ… å…±æ‰¾åˆ° {len(data)} æ¡æ¸¸æˆä¿¡æ¯ï¼š\n\n")
            for game in data:
                f.write(f"- [{game['title']}]({game['link']})")
                if game['date']:
                    f.write(f" ({game['date']})")
                if game['image']:
                    f.write(f"\n  ![å°é¢]({game['image']})")
                f.write("\n")
        else:
            f.write("âŒ å½“å‰æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ä¸ Nintendo Switch ç›¸å…³çš„æ¸¸æˆä¿¡æ¯ã€‚\n")

    print(f"ğŸ‰ æ•°æ®å·²ä¿å­˜è‡³ {json_filename} å’Œ {md_filename}")
    return json_filename, md_filename

def scrape_page(page, url, attempt=1):
    """æŠ“å–å•ä¸ªé¡µé¢"""
    try:
        # éšæœºå»¶è¿Ÿé¿å…è¢«å±è”½
        time.sleep(random.uniform(CONFIG['min_delay'], CONFIG['max_delay']))
        
        # è®¿é—®é¡µé¢
        page.goto(
            url,
            timeout=CONFIG['timeout'],
            wait_until="networkidle"  # æ›´å®½æ¾çš„ç­‰å¾…æ¡ä»¶
        )
        
        # ç­‰å¾…ä¸»è¦å†…å®¹åŠ è½½
        page.wait_for_selector("article.post", timeout=20000)
        
        # æ»šåŠ¨é¡µé¢è§¦å‘æ‡’åŠ è½½
        for _ in range(3):
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(random.uniform(1, 3))
        
        # è·å–é¡µé¢å†…å®¹
        html = page.content()
        soup = BeautifulSoup(html, 'html.parser')
        
        # è§£ææ¸¸æˆæ¡ç›®
        game_items = soup.find_all('article', class_=lambda x: x and ('post-grid' in x or 'post' in x))
        print(f"ğŸ” æ‰¾åˆ° {len(game_items)} ä¸ªæ¸¸æˆæ¡ç›®")
        
        results = []
        for item in game_items:
            try:
                title = item.find('h2', class_=lambda x: x and 'title' in x).get_text().strip()
                link = item.find('a')['href']
                
                # å¤„ç†æ—¥æœŸ
                time_tag = item.find('time')
                date_str = time_tag.get('datetime') if time_tag else ''
                formatted_date = ""
                
                if date_str:
                    try:
                        date_utc = datetime.strptime(date_str, "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=pytz.UTC)
                        date_local = date_utc.astimezone(pytz.timezone("Asia/Shanghai"))
                        formatted_date = date_local.strftime("%Y-%m-%d %H:%M")
                    except ValueError:
                        formatted_date = date_str
                
                # å¤„ç†å›¾ç‰‡
                img = item.find('img')
                image = img.get('data-src') or img.get('src') if img else ''
                
                if title and link:
                    results.append({
                        'title': title,
                        'link': link,
                        'date': formatted_date,
                        'image': image
                    })
            except Exception as e:
                print(f"âš ï¸ è§£ææ¡ç›®å¤±è´¥: {str(e)[:100]}")
                continue
        
        return results
        
    except Exception as e:
        print(f"âš ï¸ ç¬¬ {attempt} æ¬¡å°è¯•å¤±è´¥: {str(e)[:200]}...")
        if attempt < CONFIG['max_retries']:
            time.sleep(CONFIG['retry_delay'])
            return scrape_page(page, url, attempt + 1)
        else:
            print(f"âŒ é¡µé¢æŠ“å–å¤±è´¥: {url}")
            return []

def main():
    """ä¸»å‡½æ•°"""
    all_results = []
    
    with sync_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨
        browser = p.chromium.launch(
            headless=True,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--no-sandbox',
                '--disable-dev-shm-usage'  # é’ˆå¯¹Dockerç¯å¢ƒçš„ä¼˜åŒ–
            ]
        )
        
        # åˆ›å»ºä¸Šä¸‹æ–‡
        context = browser.new_context(
            user_agent=random.choice(USER_AGENTS),
            viewport={'width': 1920, 'height': 1080}
        )
        page = context.new_page()
        
        try:
            for page_num in range(1, CONFIG['pages_to_crawl'] + 1):
                url = f"{CONFIG['base_url']}/page/{page_num}" if page_num > 1 else CONFIG['base_url']
                print(f"\næ­£åœ¨è®¿é—®ç¬¬ {page_num} é¡µ: {url}")
                
                # åœ¨GitHub Actionsç¯å¢ƒä¸­ä¿å­˜è°ƒè¯•æˆªå›¾
                if os.getenv('GITHUB_ACTIONS') == 'true':
                    page.screenshot(path=f"debug_page_{page_num}.png")
                    print("å·²ä¿å­˜è°ƒè¯•æˆªå›¾")
                
                page_results = scrape_page(page, url)
                all_results.extend(page_results)
                
                # éšæœºå»¶è¿Ÿé¿å…é¢‘ç¹è¯·æ±‚
                time.sleep(random.uniform(CONFIG['min_delay'], CONFIG['max_delay']))
        
        finally:
            # ç¡®ä¿èµ„æºé‡Šæ”¾
            context.close()
            browser.close()
    
    # ä¿å­˜ç»“æœ
    return save_results(all_results)

if __name__ == "__main__":
    start_time = time.time()
    json_file, md_file = main()
    print(f"â±ï¸ æ€»è€—æ—¶: {time.time() - start_time:.2f}ç§’")
# åŸå§‹æœ‰é—®é¢˜çš„ä»£ç ï¼š
# print(f"ğŸ“Š æ€»æŠ“å–æ•°é‡: {len(json.load(open(json_file, 'r', encoding='utf-8')) if os.path.exists(json_file) else 0}")

# ä¿®æ­£åçš„ä»£ç ï¼š
if os.path.exists(json_file):
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f"ğŸ“Š æ€»æŠ“å–æ•°é‡: {len(data)}")
else:
    print("ğŸ“Š æ²¡æœ‰æŠ“å–åˆ°ä»»ä½•æ•°æ®")
